# 机器学习的几种常见的评估方法

通常，我们可通过实验测试来对学习器的泛化误差进行评估并进而做出选择。为此，需要使用一个“测试集（testing set）”来测试学习器对新样本的判别能力，然后以测试集上的“测试误差（testing error）”作为泛化误差的近似。通常我们假设测试样本也是从样本真是分布中独立同分布采样而得。但需要注意的是，测试集应该尽可能与训练集互斥，即训练样本尽量不在训练集中出现、未在训练过程中使用过。

测试样本为什么要尽可能不出现在训练集中呢？为理解这一点，不妨考虑这样一个场景：老师除了 10 道习题供同学们练习，考试时老师又用同样的这 10 道题作为试题，这个考试成绩能否有效反映出同学们学的好不好呢？答案是否定的，可能有的同学只会做这 10 道题却能得高分，回到我们的问题上来，我们希望得到泛化性能强的模型，好比是希望同学们对课程学得好、获得了对所学知识的举一反三的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试，显然，若测试样本被用作训练了，则得到的将是过于“乐观”的估计结果。

可是，我们只有一个包含 m 个样例的数据集 D={(x1,y1),(x2,y2), ..., (xm,ym)}，既要训练，又要测试，怎样才能做到呢？答案是：通过对 D 进行适当的处理，从中产生训练集 S 和测试集 T 。

## 留出法
“留出法（hold-out）”直接将数据集 D={(x1,y1),(x2,y2),...,(x100,y100)} 划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，即 D=S∪T ， S∩T=∅ 。在 S 上训练出模型后，用 T 来评估其测试误差，作为对泛化误差的估计。

以二分任务为例，假定 D 包含 1000 个样本，将其划分为 S 包含 700 个样本， T 包含 300 个样本，用 S 进行训练后，如果模型在 T 上有 90 个样本分类错误，那么其错误率为 (90/300) X 100% = 30% ，相应的，精度为 1- 30% = 70% 。

需要注意的是，训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差对最终结果产生影响，例如在分类任务中至少要保证样本的类别比例相似。如果从采样（sampling）。例如，通过对 D 进行分层采样而获得含 70% 样本的训练集 S 和含 30% 样本的测试集 T，若 D 包含 500 个正例、 500 个反例，则分层采样得到的 S 应包含 350 个正例、 350 个反例，而 T 则包含 150 个正例和 150 个反例；若 S 、T 中样本类别比例差别很大，则误差估计将由于训练/测试数据分布的差异而产生误差。

另一个需要注意的问题是，即便在给定训练/测试集的样本比例后，仍存在多种划分方式对初始数据集 D 进行分割。例如在上面的例子中，可以把 D 中的样本排序，然后把前 350 个正例放到训练集中，也可以把最后 350 个正例放到训练集中，....这些不同的划分将导致不同的训练/测试集，相应的，模型评估的结果也会有差别。因此，单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。例如 进行 100 次随机划分，每次产生一个训练/测试集用于实验评估， 100 次后就得到了 100 个结果，而留出法返回的则是这 100 个结果的平均。

此外，我们希望评估的是用 D 训练出的模型的性能，但留出法需划分训练/测试集，这就会导致一个窘境：若令训练集 S 包含绝大多数样本，则训练出的模型可能更接近于用 D 训练出的模型，但由于 T 比较小，评估结果可能不够稳定准确；若令测试集 T 多包含一些样本，则训练集 S 与 D 差别更大了，被评估的模型与用 D 训练出的模型相比可能有较大的差别，从而降低了评估结果的保真性（fidelity）。这个问题没有完美的解决方案，常见做法是将大约 2/3 ~ 4/5 的样本用于训练，剩余样本用于测试。

## 交叉验证法

“交叉验证法”（cross validation）先将数据集 D 划分为 k 个大小相似的互斥子集，即 D = D1 ∪ D2 ∪ ... ∪ Dk， Di ∩ Dj = φ (i ≠ j) 。每个子集 Di 都尽可能保持数据分布的一致性，即从 D 中通过分层采样得到。然后，每次用 k-1 个子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得 k 组训练/测试集，从而可进行 k 次训练和测试，最终返回的是这 k 个测试结果的均值。显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于 k 的取值，为强调这一点，通常把交叉验证法称为 “k 折交叉验证”（k-fold cross validation）。k 最常用的取值是 10 ，此时称为 10 折交叉验证；其他常用的 k 值有 5 、20等。下图给出了 10 折交叉验证的示意图。

![交叉验证示意图](/images/ModelAssessmentAndSelection/交叉验证法2.png)

与留出法相似，将数据集 D 划分为 k 个子集同样存在多种划分方式。为减少因样本划分不同而引入的差别， k 折交叉验证通常要随机使用不同的划分重复 p 次，最终的评估结果是这 p 次 k 折交叉验证结果的均值，例如常见的有 “10 次 10 折交叉验证”。

假定数据集 D 中包含 m 个样本，若令 k=m ，则得到了交叉验证法的一个特例：留一法不受随机样本划分方式的影响，因为 m 个样本只有唯一的的方式划分为 m 个子集 —— 每个子集包含一个样本；留一法使用的训练集与初始数据集相比只少了一个样本，这就使得在绝大多数情况下，留一法中被世纪评估的模型与期望评估的用 D 训练出的模型很相似。因此，留一法的评估结果往往被认为比较准确。然而，留一法也有其缺陷：在数据集比较大时，训练 m 个模型的计算开销可能是难以忍受的（例如数据集包含 1 百万个样本，则需训练 1 百万个模型），而这还是再未考虑算法调参的情况下。另外，留一法的估计结果也未必永远比其他评估方法准确；“没有免费的午餐”定理对实验评估方法同样适用。

## 自助法

我们希望评估的是用 D 训练出的模型，但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比 D 小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度太高了。有没有什么办法可以减少训练样本规模不同造成的影响，同时还能比较高效地进行实验估计呢？

“自助法”（bootstrapping）是一个比较好的解决方案，它直接以自助采样发（bootstrap sampling）为基础 [Efron and Tibshirani, 1993]. 给定包含 m 个样本的数据集 D ，我们对它进行采样产生数据集 D' :每次随机从 D 中挑选一个样本，将其拷贝放入 D'，然后再将该样本放回初始数据集 D 中，使得该样本在下次采样时仍有可能被采到；这样过程重复执行 m 次后，我们就得到了包含 m 个样本的数据集 D' ，这就是自助采样的结果。显然，D 中有一部分样本会在 D' 中多次出现，而另一部分样本不出现，可以做一个简单的估计，样本在 m 次采样中始终不被采到的概率是 (1- 1/m) ^m ，取极限得到

![取极限](/images/ModelAssessmentAndSelection/取极限.png)

即通过自助采样，初始数据集 D 中约有 36.8% 的样本未出现在采样数据集 D' 中，于是我们可以将 D' 用作训练集。